{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST, ImageFolder\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, RandomResizedCrop, Normalize\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import fastai.vision.augment\n",
    "import fastai.vision.data\n",
    "# from fastai.vision.data import ImageDataLoaders\n",
    "# from fastai.vision.augment import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ImageFolderDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = Compose([\n",
    "            Resize(224),\n",
    "            RandomResizedCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_dir = self.data_dir\n",
    "        transform = self.transform\n",
    "        \n",
    "        self.dls = fastai.vision.data.ImageDataLoaders.from_folder(data_dir, item_tfms=fastai.vision.augment.Resize(224))\n",
    "        self.trainset = ImageFolder(os.path.join(data_dir, 'train'), transform)\n",
    "        self.testset = ImageFolder(os.path.join(data_dir, 'valid'), transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, batch_size=self.batch_size)\n",
    "\n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.testset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Datasets/Affective_Image_Classification_Using_Features_inpired_by_Psychology_and_Art_Theory'\n",
    "dm = ImageFolderDataModule(data_dir, 32)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in dm.train_dataloader():\n",
    "    test_eq(type(x), torch.Tensor) \n",
    "    test_eq(type(y), torch.Tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(self, model=None, pretrained=False, log_level=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert model is not None, 'Select model from torchvision'\n",
    "        self.model = eval(f'torchvision.models.{model}(pretrained={pretrained})')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.model(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /home/atom/.cache/torch/hub/checkpoints/vgg11-bbd30ac9.pth\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | VGG  | 132 M \n",
      "-------------------------------\n",
      "132 M     Trainable params\n",
      "0         Non-trainable params\n",
      "132 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c375893a61024bd79064c0ad15afa9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=531456000.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b8b8c3714143dcac81e9677e5f864e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer()\n",
    "model = CNNModel('vgg11', pretrained=True)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
